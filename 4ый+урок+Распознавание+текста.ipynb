{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import sklearn as sl\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "#Параметр min_df (минимальная частота в документе) определяет, как CountVectorizer должен обходиться с редко встречающимися словами.\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = [\"How to format my hard disk\", \"Hard disk format problems\"]\n",
    "X=vectorizer.fit_transform(content)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "['How to format my hard disk', 'Hard disk format problems']\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray().transpose())\n",
    "print(content[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, features: 18\n"
     ]
    }
   ],
   "source": [
    "ddepot = \"/Users/SergeySedykh/Documents/GitHub/ST/DATA\"\n",
    "posts = [open(os.path.join(ddepot, f)).read() for f in os.listdir(ddepot)]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer(min_df=1, stop_words='english')\n",
    "\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "\n",
    "#отражаем кол-во предложений и кол-во слов:\n",
    "#print(\"#samples: %d, features: %d\" %(num_samples, num_features))\n",
    "\n",
    "print(\"#samples: %d, features: %d\" %(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'learning', 'machine', 'permanently', 'post', 'provide', 'save', 'storage', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "\n",
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function dist_raw at 0x115782d90>\n"
     ]
    }
   ],
   "source": [
    "def dist_raw (v1,v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta.toarray())\n",
    "\n",
    "print (dist_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 3.16: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 1.73: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 1.73: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist = 1.41: Imaging databases store data.\n",
      "=== Post 4 with dist = 5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3, with dist = 1.41\n"
     ]
    }
   ],
   "source": [
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "for i in range (0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec= X_train.getrow(i)\n",
    "    d=dist_raw(post_vec, new_post_vec)\n",
    "    print(\"=== Post %i with dist = %.2f: %s\" % (i, d, post))\n",
    "    if d<best_dist:\n",
    "            best_dist=d\n",
    "            best_i = i\n",
    "print(\"Best post is %i, with dist = %.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0]]\n",
      "[[0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 3 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.getrow(3).toarray())\n",
    "print(X_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 0.86: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist = 0.77: Imaging databases store data.\n",
      "=== Post 4 with dist = 0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3, with dist = 0.77\n"
     ]
    }
   ],
   "source": [
    "# Как видим, одних лишь счетчиков слов недостаточно. Необходимо нормировать векторы на едичную длину. \n",
    "# В функции dist_ raw мы будем вычислять расстояние не между исходными, а между нормированными векторами (стр. 76):\n",
    "\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "for i in range (0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec= X_train.getrow(i)\n",
    "    d=dist_norm(post_vec, new_post_vec)\n",
    "    print(\"=== Post %i with dist = %.2f: %s\" % (i, d, post))\n",
    "    if d<best_dist:\n",
    "            best_dist=d\n",
    "            best_i = i\n",
    "print(\"Best post is %i, with dist = %.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DATA_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"data\")\n",
    "DATA_DIR = os.path.join(os.path.dirname(os.path.realpath(ddepot)))\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(\"Uh, we were expecting a data directory, which contains the toy data\")\n",
    "    sys.exit(1)\n",
    "\n",
    "#print (DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphic'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=nltk.stem.SnowballStemmer('english')\n",
    "s.stem('graphics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actual', 'capabl', 'contain', 'data', 'databas', 'imag', 'interest', 'learn', 'machin', 'perman', 'post', 'provid', 'save', 'storag', 'store', 'stuff', 'toy']\n",
      "#samples: 5, features: 17\n"
     ]
    }
   ],
   "source": [
    "#Совместное использование векторизатора и стеммера из библиотеки NLTK \n",
    "\n",
    "ddepot = \"/Users/SergeySedykh/Documents/GitHub/ST/DATA\"\n",
    "posts = [open(os.path.join(ddepot, f)).read() for f in os.listdir(ddepot)]\n",
    "\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "vectorizer=StemmedCountVectorizer(min_df=1, stop_words = 'english')\n",
    "\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "\n",
    "#print(vectorizer_)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"#samples: %d, features: %d\" %(num_samples, num_features))\n",
    "\n",
    "#X_train = vectorizer.fit_transform(posts)\n",
    "#num_samples, num_features = X_train.shape\n",
    "#отражаем кол-во предложений и кол-во слов:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "\n",
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 0.63: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist = 0.77: Imaging databases store data.\n",
      "=== Post 4 with dist = 0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2, with dist = 0.63\n"
     ]
    }
   ],
   "source": [
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "for i in range (0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec= X_train.getrow(i)\n",
    "    d=dist_norm(post_vec, new_post_vec)\n",
    "    print(\"=== Post %i with dist = %.2f: %s\" % (i, d, post))\n",
    "    if d<best_dist:\n",
    "            best_dist=d\n",
    "            best_i = i\n",
    "print(\"Best post is %i, with dist = %.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
